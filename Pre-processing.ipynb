{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/development/nihars/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/development/nihars/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import nltk.tokenize as token\n",
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "import emoji\n",
    "import string\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove(\"very\")\n",
    "stop_words.add(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    # return the reviews after convering then to lowercase\n",
    "    # Words with different cases are intercepted differently such as 'The' and 'the'. \n",
    "    # Hence all words should be converted into same case, preferably lower case.\n",
    "    l = []\n",
    "    for t in text:\n",
    "        l.append(t.lower())\n",
    "    return l\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # return the reviews after removing punctuations\n",
    "    # Refer: https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html\n",
    "    l = []\n",
    "    # \\w : word character\n",
    "    # \\W : non-word character\n",
    "    # \\d : digits\n",
    "    # \\D : non-digits\n",
    "    \n",
    "    for t in text:\n",
    "        l.append(re.sub(r'[^\\w\\s]|^\\s\\d+\\s|\\s\\d+|\\d+|\\s\\d+$', ' ', t)) #|^\\s\\d+\\s|\\s\\d+|\\d+|\\s\\d+$\n",
    "    return l\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # return the reviews after removing the stopwords\n",
    "    # Stopwords are the most common words in a language. For example 'is', 'the', 'that' etc. are stopwords in English language. Stopwords shall be removed during text clean-up phase. However removing stop word can change the meaning of sentence. \n",
    "    # For instance 'I didn't love politics' will get converted to 'I love politics' after removing stopword.  \n",
    "    l = []\n",
    "    large = 0\n",
    "    for t in text:\n",
    "        word_tokens = token.word_tokenize(t)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        l.append(filtered_sentence)\n",
    "    return l\n",
    "\n",
    "def remove_URLs(text):\n",
    "    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    text= re.sub(r'[0-9]','',text)\n",
    "    return text\n",
    "\n",
    "def remove_spaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def perform_tokenization(text):\n",
    "    # return the reviews after performing tokenization\n",
    "    text = token.word_tokenize(text)\n",
    "#     tk = Whitespa/ceTokenizer()\n",
    "#     tk = WordPunctTokenizer()\n",
    "#     tk = TreebankWordTokenizer()\n",
    "#     text = tk.tokenize(text)\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "def perform_padding(data):\n",
    "    # return the reviews after padding the reviews to maximum length\n",
    "    maxlen = 30\n",
    "    return pad_sequences(data, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "def correct_spellings(text):\n",
    "    # At times textual data such as social media data is prone to spelling errors. Spelling errors \n",
    "    # should be rectified early during the clean-up phase. Fortunately we have libraries available for spelling correction.\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def convert_emoji(text):\n",
    "    text = emoji.demojize(text)\n",
    "    return text\n",
    "\n",
    "def convert_to_antonym(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    temp_word = ''\n",
    "    for word in words:\n",
    "        antonyms = []\n",
    "        if word == 'not':\n",
    "            temp_word = 'not_'\n",
    "        elif temp_word == 'not_':\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for s in syn.lemmas():\n",
    "                    for a in s.antonyms():\n",
    "                        antonyms.append(a.name())\n",
    "            if len(antonyms) >= 1:\n",
    "                word = antonyms[0]\n",
    "            else:\n",
    "                word = temp_word + word # when antonym is not found, it will\n",
    "                                    # remain not_happy\n",
    "            \n",
    "            temp_word = ''\n",
    "        if word != 'not':\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# without wordnet map it takes evey word as noun\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\n",
    "\n",
    "def stem_words(text):\n",
    "    # a process of removing and replacing suffixes to get the root form of the word.\n",
    "    # Porterstemmer is rule based. (eg: dogs -> dog)\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "def lemma_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'Python is üëç'\n",
    "# text = \"He was not happy with the score of team\"\n",
    "# text = \" David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo\"\n",
    "# text = \"This is not the most important topic\"\n",
    "text = 'Shall I search the answer in www.google.com ?'\n",
    "# text =\"Being no 1 team is more important or being no 3 but with fair play \"\n",
    "# text = \"This! sentence, contains so: many - punctuations.\"\n",
    "# text = \"Correcting   double  space  text \"\n",
    "# text = \"Spelling correctin is proprly perfrmed\"\n",
    "# text = \"Biden shouldn't interfere in India's foreign policy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shall I search the answer in  ?'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_URLs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./train.csv\"\n",
    "test_file = \"./test.csv\"\n",
    "gold_file = \"./gold_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This book was very informative, covering all a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I didn't like this product it smudged all unde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I simply love the product. I appreciate print ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It goes on very easily and makes my eyes look ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  ratings\n",
       "0  This book was very informative, covering all a...        4\n",
       "1  I am already a baseball fan and knew a bit abo...        5\n",
       "2  I didn't like this product it smudged all unde...        1\n",
       "3  I simply love the product. I appreciate print ...        5\n",
       "4  It goes on very easily and makes my eyes look ...        5"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(train_file)\n",
    "train = train.loc[:, ~train.columns.str.contains('^Unnamed')]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(temp_df):\n",
    "  temp_df['reviews'] = temp_df['reviews'].str.replace('(@\\w+\\s*)',\"\")     # remove username e.g -> @name\n",
    "  temp_df['reviews'] = temp_df['reviews'].str.replace('(&#\\w+\\s*)',\"\")     # remove html noise e.g -> &#1334\n",
    "  temp_df['reviews'] = temp_df['reviews'].str.replace('https?://[A-Za-z0-9./]+','') # remove URLs\n",
    "  temp_df['reviews'] = temp_df['reviews'].str.replace('[^\\.\\?\\w\\s]','') # remove punctuation except '.'\n",
    "  temp_df['reviews'] = temp_df['reviews'].str.replace('RT','') # remove 'RT (Retweet)'\n",
    "  temp_df['reviews'] = temp_df['reviews'].str.replace('\\n',' ') # remove '\\n'\n",
    "#   temp_df = temp_df.drop_duplicates(subset = [ 'ratings', 'reviews'])   # remove duplicate rows\n",
    "\n",
    "  return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews    it does not work((((((((((((\n",
       "ratings                               1\n",
       "Name: 49995, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[49995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews    it does not work\n",
       "ratings                   1\n",
       "Name: 49995, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = clean_df(train)\n",
    "train.iloc[49995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
