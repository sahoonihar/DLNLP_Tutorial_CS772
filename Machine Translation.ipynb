{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT using encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 456\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "spacy_tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "spacy_tokenizer_de = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "DEFIELD = data.Field(tokenize = spacy_tokenizer_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "ENFIELD = data.Field(tokenize = spacy_tokenizer_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "train_data, val_data, test_data = datasets.Multi30k.splits(exts = ('.de', '.en'), fields = (DEFIELD, ENFIELD))\n",
    "\n",
    "\n",
    "train_data, temp = train_data.split(split_ratio=0.02, random_state=random.seed(SEED))\n",
    "\n",
    "\n",
    "DEFIELD.build_vocab(train_data, min_freq = 1)\n",
    "ENFIELD.build_vocab(train_data, min_freq = 1)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim, num_hiddens, num_layers=1):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.num_hiddens = num_hiddens\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.lstm = nn.LSTM(embed_dim, num_hiddens, num_layers, batch_first=True)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    embeddings = self.embedding(inputs)\n",
    "    \n",
    "    output, (h_state, c_state) = self.lstm(embeddings, hidden)\n",
    "    return output, hidden\n",
    "\n",
    "  def init_hidden(self, batch_size=1):\n",
    "    return (torch.zeros(self.num_layers, batch_size, self.num_hiddens, device=device),\n",
    "            torch.zeros(self.num_layers, batch_size, self.num_hiddens, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, embed_dim, num_hiddens, output_size, num_layers=1):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.num_hiddens = num_hiddens\n",
    "    self.output_size = output_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(self.output_size, embed_dim)\n",
    "    self.lstm = nn.LSTM(embed_dim, self.num_hiddens, batch_first=True)\n",
    "    self.classifier = nn.Linear(self.num_hiddens, self.output_size)\n",
    "\n",
    "  def forward(self, inputs, hidden, encoder_outputs):\n",
    "    embeddings = self.embedding(inputs).view(1, -1).unsqueeze(0)\n",
    "\n",
    "    output, hidden = self.lstm(embeddings, hidden)\n",
    "    output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "    return output, hidden, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBatched(nn.Module):\n",
    "  def __init__(self, embed_dim, num_hiddens, output_size, num_layers=1):\n",
    "    super(DecoderBatched, self).__init__()\n",
    "    self.num_hiddens = num_hiddens\n",
    "    self.output_size = output_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(self.output_size, embed_dim)\n",
    "    self.lstm = nn.LSTM(embed_dim, self.num_hiddens, batch_first=True)\n",
    "    self.classifier = nn.Linear(self.num_hiddens, self.output_size)\n",
    "\n",
    "  def forward(self, inputs, hidden, encoder_outputs):\n",
    "    embeddings = self.embedding(inputs)\n",
    "\n",
    "    output, hidden = self.lstm(embeddings, hidden)\n",
    "    output = F.log_softmax(self.classifier(output), dim=2)\n",
    "    return output, hidden, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauDecoder(nn.Module):\n",
    "  def __init__(self, embed_dim, num_hiddens, output_size, num_layers=1):\n",
    "    super(BahdanauDecoder, self).__init__()\n",
    "    self.num_hiddens = num_hiddens\n",
    "    self.output_size = output_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(self.output_size, embed_dim)\n",
    "    \n",
    "    self.fc_hidden = nn.Linear(self.num_hiddens, self.num_hiddens, bias=False)\n",
    "    self.fc_encoder = nn.Linear(self.num_hiddens, self.num_hiddens, bias=False)\n",
    "    self.weight = nn.Parameter(torch.FloatTensor(1, num_hiddens))\n",
    "    self.attn_combine = nn.Linear(self.num_hiddens * 2, self.num_hiddens)\n",
    "    self.lstm = nn.LSTM(self.num_hiddens + embed_dim, self.num_hiddens, batch_first=True)\n",
    "    self.classifier = nn.Linear(self.num_hiddens, self.output_size)\n",
    "\n",
    "  def forward(self, inputs, hidden, encoder_outputs):\n",
    "    encoder_outputs = encoder_outputs.squeeze()\n",
    "    embeddings = self.embedding(inputs).view(1, -1)\n",
    "    \n",
    "    x = torch.tanh(self.fc_hidden(hidden[0])+self.fc_encoder(encoder_outputs))\n",
    "    alignment_scores = x.bmm(self.weight.unsqueeze(2))  \n",
    "    attn_weights = F.softmax(alignment_scores.view(1,-1), dim=1)\n",
    "    context_vector = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                             encoder_outputs.unsqueeze(0))\n",
    "    output = torch.cat((embeddings, context_vector[0]), 1).unsqueeze(0)\n",
    "    output, hidden = self.lstm(output, hidden)\n",
    "    output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "    return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauDecoderBatched(nn.Module):\n",
    "  def __init__(self, embed_dim, num_hiddens, output_size, num_layers=1):\n",
    "    super(BahdanauDecoderBatched, self).__init__()\n",
    "    self.num_hiddens = num_hiddens\n",
    "    self.output_size = output_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(self.output_size, embed_dim)\n",
    "    \n",
    "    self.fc_hidden = nn.Linear(self.num_hiddens, self.num_hiddens, bias=False)\n",
    "    self.fc_encoder = nn.Linear(self.num_hiddens, self.num_hiddens, bias=False)\n",
    "    self.weight = nn.Parameter(torch.FloatTensor(num_hiddens, 1))\n",
    "    self.attn_combine = nn.Linear(self.num_hiddens * 2, self.num_hiddens)\n",
    "    self.lstm = nn.LSTM(self.num_hiddens + embed_dim, self.num_hiddens, batch_first=True)\n",
    "    self.classifier = nn.Linear(self.num_hiddens, self.output_size)\n",
    "\n",
    "  def forward(self, inputs, hidden, encoder_outputs):\n",
    "    embeddings = self.embedding(inputs)\n",
    "    x = torch.tanh(self.fc_hidden(hidden[0]).squeeze(0).unsqueeze(1)+self.fc_encoder(encoder_outputs))\n",
    "    alignment_scores = x.matmul(self.weight)\n",
    "    attn_weights = F.softmax(alignment_scores, dim=1)\n",
    "    context_vector = torch.bmm(attn_weights.squeeze(2).unsqueeze(1), encoder_outputs)\n",
    "    output = torch.cat((embeddings, context_vector), 2)\n",
    "    output, hidden = self.lstm(output, hidden)\n",
    "    output = F.log_softmax(self.classifier(output), dim=2)\n",
    "    return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongDecoder(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size, attention, n_layers=1, drop_prob=0.1):\n",
    "    super(LuongDecoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.drop_prob = drop_prob\n",
    "    self.attention = attention\n",
    "    \n",
    "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "    self.dropout = nn.Dropout(self.drop_prob)\n",
    "    self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "    self.classifier = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "    \n",
    "  def forward(self, inputs, hidden, encoder_outputs):\n",
    "    embedded = self.embedding(inputs).view(1,1,-1)\n",
    "    embedded = self.dropout(embedded)\n",
    "    \n",
    "    lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "    \n",
    "    alignment_scores = self.attention(lstm_out,encoder_outputs)\n",
    "    attn_weights = F.softmax(alignment_scores.view(1,-1), dim=1)\n",
    "    context_vector = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs)\n",
    "    output = torch.cat((lstm_out, context_vector),-1)\n",
    "    output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "    return output, hidden, attn_weights\n",
    "  \n",
    "class Attention(nn.Module):\n",
    "  def __init__(self, hidden_size, method=\"dot\"):\n",
    "    super(Attention, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if method == \"general\":\n",
    "      self.fc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "      \n",
    "    elif method == \"concat\":\n",
    "      self.fc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "      self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "  \n",
    "  def forward(self, decoder_hidden, encoder_outputs):\n",
    "    if self.method == \"dot\":\n",
    "      return encoder_outputs.bmm(decoder_hidden.view(1,-1,1)).squeeze(-1)\n",
    "    \n",
    "    elif self.method == \"general\":\n",
    "      out = self.fc(decoder_hidden)\n",
    "      return encoder_outputs.bmm(out.view(1,-1,1)).squeeze(-1)\n",
    "    \n",
    "    elif self.method == \"concat\":\n",
    "      out = torch.tanh(self.fc(decoder_hidden+encoder_outputs))\n",
    "      return out.bmm(self.weight.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(ENFIELD.vocab)\n",
    "OUTPUT_SIZE = len(DEFIELD.vocab)\n",
    "EMBED_DIM = 300\n",
    "NUM_HIDDENS = 64\n",
    "NUM_LAYERS = 1 \n",
    "EPOCHS = 10 \n",
    "LR = 0.001\n",
    "\n",
    "encoder = Encoder(VOCAB_SIZE, EMBED_DIM, NUM_HIDDENS, NUM_LAYERS).to(device)\n",
    "decoder = Decoder(EMBED_DIM, NUM_HIDDENS, OUTPUT_SIZE, NUM_LAYERS).to(device)\n",
    "decoderbatched = DecoderBatched(EMBED_DIM, NUM_HIDDENS, OUTPUT_SIZE, NUM_LAYERS).to(device)\n",
    "bahdanaudecoder = BahdanauDecoder(EMBED_DIM, NUM_HIDDENS, OUTPUT_SIZE, NUM_LAYERS).to(device)\n",
    "bahdanaudecoderbatched = BahdanauDecoderBatched(EMBED_DIM, NUM_HIDDENS, OUTPUT_SIZE, NUM_LAYERS).to(device)\n",
    "\n",
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=LR)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=LR)\n",
    "decoderbatched_optimizer = torch.optim.SGD(decoderbatched.parameters(), lr=LR)\n",
    "bahdanaudecoder_optimizer = torch.optim.SGD(bahdanaudecoder.parameters(), lr=LR)\n",
    "bahdanaudecoderbatched_optimizer = torch.optim.SGD(bahdanaudecoderbatched.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92e860603624d4cad869d70213aa1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_forcing_prob = 0.5\n",
    "def train_batched(dataloader, decoder, encoder_optimizer, decoder_optimizer):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    tk0 = tqdm.notebook.tqdm(range(1,EPOCHS+1),total=EPOCHS)\n",
    "    for epoch in tk0:\n",
    "        avg_loss = 0.\n",
    "        tk1 = tqdm.notebook.tqdm(enumerate(dataloader),total=len(dataloader),leave=False)\n",
    "        for i, batch in tk1:\n",
    "            loss = 0.\n",
    "            en_inp, de_out = batch.trg, batch.src\n",
    "            h = encoder.init_hidden(en_inp.shape[0])\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_outputs, h = encoder(en_inp,h)\n",
    "\n",
    "#             decoder_input = torch.tensor(DEFIELD.vocab.stoi['<sos>'], device=device).repeat([BATCH_SIZE, 1])\n",
    "            decoder_input = de_out[:,0].unsqueeze(1)\n",
    "            decoder_hidden = h\n",
    "            output = []\n",
    "            teacher_forcing = True if random.random() < teacher_forcing_prob else False\n",
    "\n",
    "            for ii in range(1, de_out.shape[1]):\n",
    "                decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, \n",
    "                                                                       decoder_hidden, encoder_outputs)\n",
    "                top_value, top_index = decoder_output.topk(1)\n",
    "                if teacher_forcing:\n",
    "                    decoder_input = de_out[:,ii].unsqueeze(1)\n",
    "                else:\n",
    "                    decoder_input = top_index.squeeze(2)\n",
    "\n",
    "                loss += F.nll_loss(decoder_output.squeeze(1), de_out[:,ii])\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            avg_loss += loss.item()/len(dataloader)\n",
    "        tk0.set_postfix(loss=avg_loss)\n",
    "        \n",
    "train_batched(train_dataloader, bahdanaudecoderbatched, encoder_optimizer, bahdanaudecoderbatched_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da11a4ff83c4597adb68997b1f6383b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_batched(train_dataloader, decoderbatched, encoder_optimizer, decoderbatched_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Actual: {}\".format(' '.join(list(map(lambda x: DEFIELD.vocab.itos[x], de_out[0])))))\n",
    "print(\"English: {}\".format(' '.join(list(map(lambda x: ENFIELD.vocab.itos[x], en_inp[0])))))\n",
    "print(\"Predicted: {}\".format(' '.join(list(map(lambda x: DEFIELD.vocab.itos[x], output[:25])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "en_inp, de_out = batch.trg, batch.src\n",
    "h = encoder.init_hidden(1)\n",
    "encoder_outputs, h = encoder(en_inp[0].unsqueeze(0),h)\n",
    "\n",
    "decoder_input = de_out[0,0].unsqueeze(0).unsqueeze(0)\n",
    "decoder_hidden = h\n",
    "\n",
    "output = []\n",
    "attentions = []\n",
    "while True:\n",
    "    \n",
    "    decoder_output, decoder_hidden, attn_weights = bahdanaudecoderbatched(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    _, top_index = decoder_output.topk(1)\n",
    "    decoder_input = top_index.squeeze(2)\n",
    "    print(top_index.item())\n",
    "    \n",
    "    \n",
    "    if top_index.item() == DEFIELD.vocab.stoi[\"<eos>\"]:\n",
    "        break\n",
    "    output.append(top_index.item())\n",
    "#     attentions.append(attn_weights.squeeze().cpu().detach().numpy())\n",
    "#     print(\"Actual: {}\".format(' '.join(list(map(lambda x: DEFIELD.vocab.itos[x], de_out[0])))))\n",
    "#     print(\"English: {}\".format(' '.join(list(map(lambda x: ENFIELD.vocab.itos[x], en_inp[0])))))\n",
    "#     print(\"Predicted: {}\".format(' '.join(list(map(lambda x: DEFIELD.vocab.itos[x], de_out[0])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e323ff2c4c14151b70e42c0d26ae61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_forcing_prob = 0.5\n",
    "def train(dataloader, decoder, encoder_optimizer, decoder_optimizer):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    tk0 = tqdm.notebook.tqdm(range(1,EPOCHS+1),total=EPOCHS)\n",
    "    for epoch in tk0:\n",
    "        avg_loss = 0.\n",
    "        tk1 = tqdm.notebook.tqdm(enumerate(dataloader),total=len(dataloader),leave=False)\n",
    "        for i, batch in tk1:\n",
    "            loss = 0.\n",
    "            h = encoder.init_hidden()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            en_inp, de_out = batch.trg, batch.src\n",
    "            encoder_outputs, h = encoder(en_inp,h)\n",
    "\n",
    "\n",
    "            decoder_input = torch.tensor([[DEFIELD.vocab.stoi['<sos>']]], device=device)\n",
    "            decoder_hidden = h\n",
    "            output = []\n",
    "            teacher_forcing = True if random.random() < teacher_forcing_prob else False\n",
    "\n",
    "            for ii in range(1, de_out.shape[1]):\n",
    "                decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, \n",
    "                                                                       decoder_hidden, encoder_outputs)\n",
    "                top_value, top_index = decoder_output.topk(1)\n",
    "                if teacher_forcing:\n",
    "                    decoder_input = torch.tensor([de_out[0][ii].item()],device=device)\n",
    "                else:\n",
    "                    decoder_input = torch.tensor([top_index.item()],device=device)\n",
    "\n",
    "                loss += F.nll_loss(decoder_output.view(1,-1), torch.tensor([de_out[0][ii].item()],device=device))\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            avg_loss += loss.item()/len(dataloader)\n",
    "        tk0.set_postfix(loss=avg_loss)\n",
    "        \n",
    "train(train_dataloader, bahdanaudecoder, encoder_optimizer, bahdanaudecoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "h = encoder.init_hidden()\n",
    "en_inp, de_out = batch.trg, batch.src\n",
    "encoder_outputs, h = encoder(en_inp,h)\n",
    "\n",
    "\n",
    "\n",
    "decoder_input = torch.tensor([[DEFIELD.vocab.stoi['<sos>']]], device=device)\n",
    "decoder_hidden = h\n",
    "output = []\n",
    "attentions = []\n",
    "i = 0\n",
    "while True:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    _, top_index = decoder_output.topk(1)\n",
    "    decoder_input = torch.tensor([top_index.item()],device=device)\n",
    "    if top_index.item() == DEFIELD.vocab.stoi[\"<eos>\"]:\n",
    "        break\n",
    "    output.append(top_index.item())\n",
    "#     attentions.append(attn_weights.squeeze().cpu().detach().numpy())\n",
    "print(\"Actual: {}\".format(' '.join(list(map(lambda x: DEFIELD.vocab.itos[x], de_out)))))\n",
    "print(\"English: {}\".format(' '.join(list(map(lambda x: ENFIELD.vocab.itos[x], en_inp)))))\n",
    "print(\"Predicted: {}\".format(' '.join(list(map(lambda x: DEFIELD.vocab.itos[x], output)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
