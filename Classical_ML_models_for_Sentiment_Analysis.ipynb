{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classical ML models for Sentiment Analysis",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis using ML models"
      ],
      "metadata": {
        "id": "JDDP6tVvSebN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary libraries - using sklearn "
      ],
      "metadata": {
        "id": "fO8O2fEXSnJC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTELKbAeZznp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# text preprocessing\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# feature extraction / vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# classifiers\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# save and load a file\n",
        "import pickle\n",
        "import nltk\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RniTmCyZdE_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06d326e-d32b-47b7-d859-59e4891c1178"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za-U6dnDaBuc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "266696b6-f618-43ca-ed98-85a7d6256f62"
      },
      "source": [
        "df_train = pd.read_csv('/content/S_train_with_sentences.csv')\n",
        "df_test = pd.read_csv('/content/S_test_with_sentences.csv')\n",
        "\n",
        "X_train = df_train.reviews\n",
        "X_test = df_test.reviews\n",
        "\n",
        "y_train = df_train.sentiment\n",
        "y_test = df_test.sentiment\n",
        "\n",
        "#class_names = ['joy', 'sadness', 'anger', 'neutral', 'fear']\n",
        "data = pd.concat([df_train, df_test])\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9408, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                            reviews  sentiment\n",
              "0           0  Apparently Prides Osteria had a rough summer a...          1\n",
              "1           1  However new blood in the kitchen seems to have...          2\n",
              "2           2                Waitstaff was warm but unobtrusive.          2\n",
              "3           3  By 8 pm or so when we left the bar was full an...          1\n",
              "4           4  After reading the mixed reviews of late I was ...          2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2a7bca6-2a05-4f14-861f-c18b83f0bb04\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>reviews</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Apparently Prides Osteria had a rough summer a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>However new blood in the kitchen seems to have...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Waitstaff was warm but unobtrusive.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>By 8 pm or so when we left the bar was full an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>After reading the mixed reviews of late I was ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2a7bca6-2a05-4f14-861f-c18b83f0bb04')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f2a7bca6-2a05-4f14-861f-c18b83f0bb04 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f2a7bca6-2a05-4f14-861f-c18b83f0bb04');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiPA7IhtsdpN",
        "outputId": "3003aee0-bf97-4c58-c7da-a9aee24c3974"
      },
      "source": [
        "type(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87PtYG4Xbo4Q"
      },
      "source": [
        "def preprocess_and_tokenize(data):    \n",
        "\n",
        "    #remove html markup\n",
        "    data = re.sub(\"(<.*?>)\", \"\", data)\n",
        "\n",
        "    #remove urls\n",
        "    data = re.sub(r'http\\S+', '', data)\n",
        "    \n",
        "    #remove hashtags and @names\n",
        "    data= re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
        "    data= re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
        "\n",
        "    #remove punctuation and non-ascii digits\n",
        "    data = re.sub(\"(\\\\W|\\\\d)\", \" \", data)\n",
        "    \n",
        "    #remove whitespace\n",
        "    data = data.strip()\n",
        "    \n",
        "    # tokenization with nltk\n",
        "    data = word_tokenize(data)\n",
        "    \n",
        "    # stemming with nltk\n",
        "    porter = PorterStemmer()\n",
        "    stem_data = [porter.stem(word) for word in data]\n",
        "        \n",
        "    return stem_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF vectorization"
      ],
      "metadata": {
        "id": "FDg-F4JzEQ9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by $t$, a document by $d$, and the corpus by $D$. Term frequency $T F(t, d)$ is the number of times that term $t$ appears in document $d$, while document frequency $D F(t, D)$ is the number of documents that contains term $t$. If we only use term frequency to measure the importance, it is very easy to overemphasize terms that appear very often but carry little information about the document, e.g., \"a\", \"the\", and \"of\". If a term appears very often across the corpus, it means it doesn't carry special information about a particular document. Inverse document frequency is a numerical measure of how much information a term provides:\n",
        "$$\n",
        "I D F(t, D)=\\log \\frac{|D|+1}{D F(t, D)+1}\n",
        "$$\n",
        "where $|D|$ is the total number of documents in the corpus. Since logarithm is used, if a term appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n",
        "$$\n",
        "T F I D F(t, d, D)=T F(t, d) \\cdot I D F(t, D)\n",
        "$$"
      ],
      "metadata": {
        "id": "GYn-ud_MEGo1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VhFZgv4c1iI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc6d4e29-a140-4559-cf8f-b33927c35dbb"
      },
      "source": [
        "vect = TfidfVectorizer(tokenizer=preprocess_and_tokenize, sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
        "\n",
        "# fit on our complete corpus\n",
        "X = vect.fit_transform(data.reviews)\n",
        "print(X.shape)\n",
        "#print(vect.vocabulary_)\n",
        "# transform testing and training datasets to vectors\n",
        "X_train_vect = vect.transform(X_train)\n",
        "X_test_vect = vect.transform(X_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9408, 51930)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"n_samples: %d, n_features: %d\" % X_train_vect.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2imVkHFeUvub",
        "outputId": "0cc965ab-38c4-4dca-cd0a-171cdd51e7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_samples: 8998, n_features: 51930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine Classification\n",
        "\n",
        "\n",
        "## What are some use cases for SVMs?\n",
        "\n",
        "-Classification, regression (time series prediction, etc) , outlier detection, clustering\n",
        "\n",
        "\n",
        "## How does an SVM compare to other ML algorithms?\n",
        "\n",
        "![alt text](https://image.slidesharecdn.com/mscpresentation-140722065852-phpapp01/95/msc-presentation-bioinformatics-7-638.jpg?cb=1406012610 \"Logo Title Text 1\")\n",
        "\n",
        "- As a rule of thumb, SVMs are great for relatively small data sets with fewer outliers. \n",
        "- Other algorithms (Random forests, deep neural networks, etc.) require more data but almost always come up with very robust models.\n",
        "- The decision of which classifier to use depends on your dataset and the general complexity of the problem.\n",
        "- \"Premature optimization is the root of all evil (or at least most of it) in programming.\" - Donald Knuth, CS Professor (Turing award speech 1974)  \n",
        "\n",
        "\n",
        "## What is a Support Vector Machine?\n",
        "\n",
        "It's a supervised machine learning algorithm which can be used for both classification or regression problems. But it's usually used for classification. Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n",
        "\n",
        "## What are Support Vectors?\n",
        "\n",
        "![alt text](https://www.dtreg.com/uploaded/pageimg/SvmMargin2.jpg \"Logo Title Text 1\")\n",
        " \n",
        "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set, they are what help us build our SVM. \n",
        "\n",
        "## Whats a hyperplane?\n",
        "\n",
        "![alt text](http://slideplayer.com/slide/1579281/5/images/32/Hyperplanes+as+decision+surfaces.jpg \"Logo Title Text 1\")\n",
        "\n",
        "Geometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with dimension n − 1. By its nature, it separates the space into two half spaces.\n",
        "\n",
        "## Let's define our loss function (what to minimize) and our objective function (what to optimize)\n",
        "\n",
        "#### Loss function\n",
        "\n",
        "We'll use the Hinge loss. This is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs).\n",
        "\n",
        "![alt text](http://i.imgur.com/OzCwzyN.png \"Logo Title Text 1\")\n",
        "\n",
        "\n",
        "c is the loss function, x the sample, y is the true label, f(x) the predicted label.\n",
        "\n",
        "![alt text](http://i.imgur.com/FZ7JcG3.png \"Logo Title Text 1\")\n",
        "\n",
        " \n",
        "#### Objective Function\n",
        "\n",
        "![alt text](http://i.imgur.com/I5NNu44.png \"Logo Title Text 1\")\n",
        "\n",
        "As you can see, our objective of a SVM consists of two terms. The first term is a regularizer, the heart of the SVM, the second term the loss. The regularizer balances between margin maximization and loss. We want to find the decision surface that is maximally far away from any data points.\n",
        "\n",
        "How do we minimize our loss/optimize for our objective (i.e learn)?\n",
        "\n",
        "We have to derive our objective function to get the gradients! Gradient descent ftw.  As we have two terms, we will derive them seperately using the sum rule in differentiation.\n",
        "\n",
        "\n",
        "![alt text](http://i.imgur.com/6uK3BnH.png \"Logo Title Text 1\")\n",
        "\n",
        "This means, if we have a misclassified sample, we update the weight vector w using the gradients of both terms, else if classified correctly,we just update w by the gradient of the regularizer.\n",
        "\n",
        "\n",
        "\n",
        "Misclassification condition \n",
        "\n",
        "![alt text](http://i.imgur.com/g9QLAyn.png \"Logo Title Text 1\")\n",
        "\n",
        "Update rule for our weights (misclassified)\n",
        "\n",
        "![alt text](http://i.imgur.com/rkdPpTZ.png \"Logo Title Text 1\")\n",
        "\n",
        "including the learning rate η and the regularizer λ\n",
        "The learning rate is the length of the steps the algorithm makes down the gradient on the error curve.\n",
        "- Learning rate too high? The algorithm might overshoot the optimal point.\n",
        "- Learning rate too low? Could take too long to converge. Or never converge.\n",
        "\n",
        "The regularizer controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data. As a regulizing parameter we choose 1/epochs, so this parameter will decrease, as the number of epochs increases.\n",
        "- Regularizer too high? overfit (large testing error) \n",
        "- Regularizer too low? underfit (large training error) \n",
        "\n",
        "Update rule for our weights (correctly classified)\n",
        "\n",
        "![alt text](http://i.imgur.com/xTKbvZ6.png \"Logo Title Text 1\")\n",
        "\n",
        "The above theory snippet is taken from \n",
        "#### Reference: https://colab.research.google.com/github/akshayrb22/playing-with-data/blob/master/supervised_learning/support_vector_machine/svm.ipynb"
      ],
      "metadata": {
        "id": "xtb12v7DiJBm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WbjbfnAdXyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572c01da-a244-4c6c-8246-6acdc35f7d9a"
      },
      "source": [
        "svc = LinearSVC(tol=1e-05)\n",
        "svc.fit(X_train_vect, y_train)\n",
        "\n",
        "ysvm_pred = svc.predict(X_test_vect)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, ysvm_pred) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, ysvm_pred, average='micro') * 100))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, ysvm_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 74.88%\n",
            "\n",
            "F1 Score: 74.88\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 93  14  19]\n",
            " [ 21 104  43]\n",
            " [  0   6 110]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFZ9loZmrq8N",
        "outputId": "cca60984-a0a3-42b4-bfbb-cd36791761bc"
      },
      "source": [
        "sent = \"Food was best\"\n",
        "sent = pd.Series(sent)\n",
        "sent_test=vect.transform(sent)\n",
        "\n",
        "svc.predict(sent_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Classifier in Python - Basic Introduction\n",
        "\n",
        "\n",
        "\n",
        "In logistic regression... basically, you are performing linear regression but applying a sigmoid function for the outcome.\n",
        "\n",
        "#### Sigmoid  / Logistic Function\n",
        "\n",
        "$p =1 / 1 + e^{-y}$\n",
        "\n",
        "#### Properties of Logistic Regression\n",
        "\n",
        "* The dependent variable follows a Bernoulli Distribution\n",
        "* Estimation is maximum likelihood estimation (MLE)\n",
        "\n",
        "#### Advantages\n",
        "* Straight forward, easy to implement, doesn't require high compute power, easy to interpret, used widely. \n",
        "* Doesn't require feature scaling and provides a probability score for observations.\n",
        "\n",
        "#### Disadvantages\n",
        "* Not able to handle a large number of category features/variables. \n",
        "* Vulnerable to overfitting. "
      ],
      "metadata": {
        "id": "SmnZWojhmULW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0lQBbTveB6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50317c9c-96a5-43cb-91f4-a9cbc93d63a5"
      },
      "source": [
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(X_train_vect, y_train)\n",
        "ylgr_pred=logisticRegr.predict(X_test_vect)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, ylgr_pred) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, ylgr_pred, average='micro') * 100))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, ylgr_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 69.27%\n",
            "\n",
            "F1 Score: 69.27\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 83  14  29]\n",
            " [ 20  88  60]\n",
            " [  0   3 113]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes classifier"
      ],
      "metadata": {
        "id": "ixYLNc1-Emms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem. This section will focus on an intuitive explanation of how naive Bayes classifiers work, followed by a couple examples of them in action on some datasets."
      ],
      "metadata": {
        "id": "3AwZlZnVEp6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes classifiers are built on Bayesian classification methods.\n",
        "These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.\n",
        "In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\\rm features})$.\n",
        "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
        "\n",
        "$$\n",
        "P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n",
        "$$\n",
        "\n",
        "If we are trying to decide between two labels—let's call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
        "\n",
        "$$\n",
        "\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
        "$$\n",
        "\n",
        "All we need now is some model by which we can compute $P({\\rm features}~|~L_i)$ for each label.\n",
        "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
        "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
        "The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
        "\n",
        "This is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification."
      ],
      "metadata": {
        "id": "abXqHtOqEwBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multinomial Naive Bayes\n",
        "\n",
        "The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.\n",
        "Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution.\n",
        "The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates."
      ],
      "metadata": {
        "id": "jfrLil5yGQcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = MultinomialNB()\n",
        "Model.fit(X_train_vect, y_train)\n",
        "NB_pred=Model.predict(X_test_vect)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, NB_pred) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, NB_pred, average='micro') * 100))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, NB_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPGmkTvzFAwW",
        "outputId": "cbe9f376-da36-4ea4-e83c-1a2b7a00de75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 37.56%\n",
            "\n",
            "F1 Score: 37.56\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 30   1  95]\n",
            " [  7   8 153]\n",
            " [  0   0 116]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfQ5YHL8fP1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26442a79-5eac-4151-fa27-b15eef4f155d"
      },
      "source": [
        "svclassifier = SVC(kernel='linear')\n",
        "svclassifier.fit(X_train_vect, y_train)\n",
        "svm_pred=svclassifier.predict(X_test_vect)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, svm_pred) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, svm_pred, average='micro') * 100))\n",
        "print(\"\\nCOnfusion Matrix:\\n\", confusion_matrix(y_test, svm_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 74.39%\n",
            "\n",
            "F1 Score: 74.39\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 94  16  16]\n",
            " [ 23 104  41]\n",
            " [  2   7 107]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ],
      "metadata": {
        "id": "mzA3BxccI6F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=50)\n",
        "rf.fit(X_train_vect, y_train)\n",
        "\n",
        "yrf_pred = rf.predict(X_test_vect)\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, yrf_pred) * 100))\n",
        "print(\"\\nF1 Score: {:.2f}\".format(f1_score(y_test, yrf_pred, average='micro') * 100))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, yrf_pred))"
      ],
      "metadata": {
        "id": "etACYG5IDjeX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}